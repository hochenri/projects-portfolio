{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1fe505-be53-4ee4-b0e9-8fe38c2aa065",
   "metadata": {},
   "source": [
    "# ETL Job for Diagnosis Dataset Production\n",
    "This is a simple project I worked on to produce a dataset with diagnosis data by fetching the required data from database tables in Amazon Redshift.\n",
    "\n",
    "**Problem**: since the database tables contained a large number of records, querying the database with redshift-connector library was taking too much time.\n",
    "\n",
    "**Solution**: introduced Spark in the analysis environment used by the business stakeholders.\n",
    "\n",
    "**Note**: since this was an actual business application, I'll report only part of the project I worked on, hiding all sensitive information.\n",
    "\n",
    "The following was the Python script used to retrieve data from the database tables and produce a dataset with the obtained dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e6307-c372-4ad8-9dfb-42bd3412a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs\")\n",
    "    parser.add_argument(\"--dbname\", type=str, help=\"database name\")\n",
    "    parser.add_argument(\"--schema\", type=str, help=\"redshift schema\")\n",
    "    parser.add_argument(\"--host\", type=str, help=\"redshift database host\")\n",
    "    parser.add_argument(\"--port\", type=str, help=\"redshift port\")\n",
    "    parser.add_argument(\"--username\", type=str, help=\"redshift username\")\n",
    "    parser.add_argument(\"--password\", type=str, help=\"redshift password\")\n",
    "    parser.add_argument(\"--tempdir\", type=str, help=\"s3 temp bucket uri\")\n",
    "    parser.add_argument(\"--driver\", type=str, help=\"jdbc driver\")\n",
    "    parser.add_argument(\"--dependencies\", type=str, help=\"dependencies for Spark-Redshift connection\")\n",
    "    parser.add_argument(\"--role\", type=str, help=\"AWS IAM role\")\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.jars\", args.driver) \\\n",
    "        .config(\"spark.jars.packages\", args.dependencies) \\\n",
    "        .getOrCreate()\n",
    "    sql_context = SQLContext(spark.sparkContext)\n",
    "\n",
    "    # URL composition\n",
    "    url = f\"jdbc:redshift://{args.host}:{args.port}/{args.dbname}?user={args.username}&password={args.password}\"\n",
    "\n",
    "    # Spark-Redshift connection format\n",
    "    connection_format = \"io.github.spark_redshift_community.spark.redshift\"\n",
    "\n",
    "    # First DB query\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        d.member_id,\n",
    "        d.date_visit,\n",
    "        m.level1_code,\n",
    "        m.level2_code\n",
    "    FROM {args.schema}.diagnosis AS d\n",
    "    INNER JOIN {args.schema}.diagnosis_master AS m\n",
    "    ON d.diagnosis_code = m.diagnosis_code\n",
    "    WHERE m.level1_code in ('ABC', 'DEF', 'GHI')\n",
    "    ORDER BY\n",
    "        d.member_id,\n",
    "        d.date_visit,\n",
    "        m.level1_code,\n",
    "        m.level2_code\n",
    "    \"\"\"\n",
    "    diagnosis_df = sql_context.read \\\n",
    "        .format(connection_format) \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"tempdir\", args.tempdir) \\\n",
    "        .option(\"aws_iam_role\", args.role) \\\n",
    "        .load()\n",
    "\n",
    "    # Second DB query\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            member_id,\n",
    "            date_checkup,\n",
    "            bmi\n",
    "        FROM {args.schema} health_checkup\n",
    "        WHERE bmi >= 27\n",
    "    \"\"\"\n",
    "    bmi_df = sql_context.read \\\n",
    "        .format(connection_format) \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .option(\"tempdir\", args.tempdir) \\\n",
    "        .option(\"aws_iam_role\", args.role) \\\n",
    "        .load()\n",
    "\n",
    "    # Dataset creation\n",
    "    bmi_diagnosis = diagnosis_df.join(\n",
    "        bmi_df,\n",
    "        diagnosis_df[\"member_id\"] == bmi_df[\"member_id\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # Rest of code\n",
    "    # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
